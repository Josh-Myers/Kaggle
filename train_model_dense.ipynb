{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.applications.densenet.DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and eval metrics\n",
    "# https://www.kaggle.com/akensert/resnet50-keras-baseline-model\n",
    "from keras import backend as K\n",
    "\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Can be used as the loss function in model.compile()\n",
    "    ---------------------------------------------------\n",
    "    \"\"\"\n",
    "    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "    out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    return K.mean(out, axis=-1)\n",
    "\n",
    "def _normalized_weighted_average(arr, weights=None):\n",
    "    \"\"\"\n",
    "    A simple Keras implementation that mimics that of \n",
    "    numpy.average(), specifically for the this competition\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        scl = K.sum(weights)\n",
    "        weights = K.expand_dims(weights, axis=1)\n",
    "        return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "    return K.mean(arr, axis=1)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Will be used as the metric in model.compile()\n",
    "    ---------------------------------------------\n",
    "    Similar to the custom loss function 'weighted_log_loss()' above\n",
    "    but with normalized weights, which should be very similar \n",
    "    to the official competition metric:\n",
    "        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
    "    and hence:\n",
    "        sklearn.metrics.log_loss with sample weights\n",
    "    \"\"\"\n",
    "    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "    loss = -(        y_true  * K.log(      y_pred)\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    return K.mean(loss_samples)\n",
    "\n",
    "def weighted_log_loss_metric(trues, preds):\n",
    "    \"\"\"\n",
    "    Will be used to calculate the log loss \n",
    "    of the validation set in PredictionCheckpoint()\n",
    "    ------------------------------------------\n",
    "    \"\"\"\n",
    "    class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    epsilon = 1e-7\n",
    "    preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "    loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "    loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "    return - loss_samples.mean()\n",
    "\n",
    "#need to use callback to save model\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras', save_best_only=True)\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "# print log to file\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "val = pd.read_csv('val.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(538630, 8) (68290, 8) (67337, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674257"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "538630 + 68290 + 67337 # 674257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>PatientID</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_231d901c1.jpg</td>\n",
       "      <td>ID_b81a287f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_127689cce.jpg</td>\n",
       "      <td>ID_42910d3d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_25457734a.jpg</td>\n",
       "      <td>ID_329aafa7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_81c9aa125.jpg</td>\n",
       "      <td>ID_6b544c3c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>ID_87e8b2528.jpg</td>\n",
       "      <td>ID_d6e578fb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename    PatientID  any  epidural  intraparenchymal  \\\n",
       "0  ID_231d901c1.jpg  ID_b81a287f    1         0                 0   \n",
       "2  ID_127689cce.jpg  ID_42910d3d    0         0                 0   \n",
       "3  ID_25457734a.jpg  ID_329aafa7    0         0                 0   \n",
       "4  ID_81c9aa125.jpg  ID_6b544c3c    0         0                 0   \n",
       "5  ID_87e8b2528.jpg  ID_d6e578fb    0         0                 0   \n",
       "\n",
       "   intraventricular  subarachnoid  subdural  \n",
       "0                 0             1         0  \n",
       "2                 0             0         0  \n",
       "3                 0             0         0  \n",
       "4                 0             0         0  \n",
       "5                 0             0         0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() # they should be int not float\n",
    "# need to add the .jpg to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606920, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no test set - combine train and val\n",
    "train = pd.concat([train, val], ignore_index=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create submission df\n",
    "submission = pd.read_csv('stage_1_sample_submission.csv')\n",
    "submission[\"Image\"] = submission[\"ID\"].str.slice(stop=12) + '.jpg'\n",
    "submission[\"Diagnosis\"] = submission[\"ID\"].str.slice(start=13)\n",
    "submission = submission.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "submission = submission.set_index(['Image', 'Diagnosis']).unstack(level=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = submission.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Image</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_000012eaf.jpg</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_0000ca2f6.jpg</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_000259ccf.jpg</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_0002d438a.jpg</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_00032d440.jpg</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Diagnosis             Image  any  epidural  intraparenchymal  \\\n",
       "0          ID_000012eaf.jpg  0.5       0.5               0.5   \n",
       "1          ID_0000ca2f6.jpg  0.5       0.5               0.5   \n",
       "2          ID_000259ccf.jpg  0.5       0.5               0.5   \n",
       "3          ID_0002d438a.jpg  0.5       0.5               0.5   \n",
       "4          ID_00032d440.jpg  0.5       0.5               0.5   \n",
       "\n",
       "Diagnosis  intraventricular  subarachnoid  subdural  \n",
       "0                       0.5           0.5       0.5  \n",
       "1                       0.5           0.5       0.5  \n",
       "2                       0.5           0.5       0.5  \n",
       "3                       0.5           0.5       0.5  \n",
       "4                       0.5           0.5       0.5  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78545"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission) # 78545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "im_size = 224\n",
    "seed = 42\n",
    "columns=[\"any\", \"epidural\", \"intraparenchymal\", \"intraventricular\", \"subarachnoid\", \"subdural\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 606920 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory='/home/jupyter/train_images_bsb_224/',\n",
    "    x_col=\"filename\",\n",
    "    y_col=columns,\n",
    "    target_size=(im_size, im_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='other',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67337 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    directory='/home/jupyter/train_images_bsb_224/',\n",
    "    x_col=\"filename\",\n",
    "    y_col=columns,\n",
    "    target_size=(im_size, im_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='other',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78545 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "submission_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=submission,\n",
    "    directory='/home/jupyter/test_images_bsb_224/', # this one is different dir\n",
    "    x_col=\"Image\",\n",
    "    y_col=None,\n",
    "    target_size=(im_size, im_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = train_generator.n // train_generator.batch_size\n",
    "num_val_steps = val_generator.n // val_generator.batch_size\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = DenseNet121(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = pre_trained_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# add a fully-connected layer\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  \n",
    "predictions = tf.keras.layers.Dense(6, activation='sigmoid')(x)\n",
    "model = Model(inputs=pre_trained_model.input, outputs=predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use default lr to train top\n",
    "model.compile(optimizer='rmsprop', loss=weighted_log_loss, metrics=[weighted_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9483/9483 [==============================] - 7127s 752ms/step - loss: 0.2012 - weighted_loss: 0.1725 - val_loss: 0.7494 - val_weighted_loss: 0.6424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b74e9bdd8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_steps,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=num_val_steps,\n",
    "    callbacks=[checkpointer, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# load model after training top\n",
    "model = tf.keras.models.load_model('my_model.keras', custom_objects={'weighted_log_loss': weighted_log_loss, 'weighted_loss': weighted_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = train_generator.n // train_generator.batch_size\n",
    "num_val_steps = val_generator.n // val_generator.batch_size\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now recomplie use adam with small lr to train lower layers \n",
    "model.compile(optimizer=Adam(lr=0.0001), loss=weighted_log_loss, metrics=[weighted_loss]) # change to 0.0005 if too slow to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "9483/9483 [==============================] - 7140s 753ms/step - loss: 0.1166 - weighted_loss: 0.0999 - val_loss: 0.0984 - val_weighted_loss: 0.0843\n",
      "Epoch 2/40\n",
      "9483/9483 [==============================] - 7138s 753ms/step - loss: 0.0946 - weighted_loss: 0.0811 - val_loss: 0.0947 - val_weighted_loss: 0.0812\n",
      "Epoch 3/40\n",
      "2278/9483 [======>.......................] - ETA: 1:18:48 - loss: 0.0884 - weighted_loss: 0.0757"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_steps,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=num_val_steps,\n",
    "    callbacks=[checkpointer, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if too big use the smaller one - 121"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
